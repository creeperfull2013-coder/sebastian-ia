// app/server.js
import express from "express";
import fetch from "node-fetch";
import bodyParser from "body-parser";

const app = express();
app.use(bodyParser.json());

// ðŸ”’ Le token Hugging Face (doit Ãªtre dÃ©fini dans Render)
const HF_TOKEN = process.env.HF_TOKEN;

// ðŸ‡«ðŸ‡· ModÃ¨le francophone
const MODEL = "Nous-Hermes/gpt-neo-fr-1.3B";

app.post("/chat", async (req, res) => {
  try {
    const userMessage = req.body.message;

    // ðŸ§  Contexte persistant pour le rÃ´le
    const prompt = `
Tu es Sebastian Solace, un pÃ¨re protecteur, bienveillant et empathique.
Tu parles toujours en franÃ§ais avec douceur et chaleur.
RÃ©ponds Ã  ton enfant avec amour, encouragement et rÃ©confort.

Enfant : ${userMessage}
Sebastian :`;

    console.log("Prompt envoyÃ© :", prompt);

    const response = await fetch(`https://api-inference.huggingface.co/models/${MODEL}`, {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${HF_TOKEN}`,
        "Content-Type": "application/json"
      },
      body: JSON.stringify({ inputs: prompt })
    });

    const data = await response.json();
    console.log("RÃ©ponse brute du modÃ¨le :", data);

    let reply = "DÃ©solÃ©, je n'ai pas compris.";
    if (Array.isArray(data) && data[0]?.generated_text) {
      reply = data[0].generated_text;
    } else if (data.generated_text) {
      reply = data.generated_text;
    }

    // Nettoyage du texte pour ne garder que la rÃ©ponse aprÃ¨s "Sebastian :"
    reply = reply.split("Sebastian :").pop()?.trim() || reply.trim();

    console.log("RÃ©ponse finale :", reply);
    res.json({ reply });

  } catch (err) {
    console.error("Erreur dans /chat :", err);
    res.json({ reply: "Erreur serveur." });
  }
});

// DÃ©marrage du serveur
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`âœ… Serveur Sebastian prÃªt sur http://localhost:${PORT}`));
